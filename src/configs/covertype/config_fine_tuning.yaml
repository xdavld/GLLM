General:
  operation: "fine-tuning"
  model_name_or_path: "../models/Llama-3.2-3B-Instruct/Base"

Data:
  prompt_template_path: "templates/generator.txt"
  sample_size: 1
  output_size: 1
  dataset_path: "data/public/processed/covertype/train.csv"
  dataset_splits:
    - "train"
    - "eval"
  eval_size: 0.2
  test_size: 0.1
  seed: 42

Training:
  output_dir: "../models/Llama-3.2-3B-Instruct/FineTuned/Covertype"
  learning_rate: 5e-5
  fp16: True
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  num_train_epochs: 5
  padding_free: True
  gradient_accumulation_steps: 1
  eval_strategy: "epoch"
  save_strategy: "epoch"
  load_best_model_at_end: True
  metric_for_best_model: "eval_loss"
  greater_is_better: False
  save_total_limit: 10