# GLLM: A GAN-Inspired LLM Framework for the Synthetic Generation of Tabular Datasets

## Description
Generative framework that integrates Generative Adversarial Network (GAN) principles with Large Language Models (LLMs) to synthetically augment small-scale tabular datasets.

## 1. Fine-Tuning the Generator

Set up and run the following to fine-tune the generator model so it learns to capture the domain-specific structure and statistical relationships:

```bash
export TOKENIZERS_PARALLELISM=false
accelerate launch --config_file configs/deepspeed_zero3.yaml main.py configs/<DATASET>/config_fine_tuning.yaml
```

## 2. GLLM Training, Synthetic Data Generation, and Evaluation

Follow this workflow to train the generalized GLLM using an adversarial setup, generate synthetic tabular data, and assess model performance:

1. Change directory to the `src` folder:

   ```bash
   cd src
   ```
2. Adjust file paths or hyperparameters in the config files as needed.
3. Run the main script with the desired config file:

   ```bash
   python main.py configs/<DATASET>/<CONFIG_FILE>
   ```

   For example:

   ```bash
   python main.py configs/beer/config_fine_tuning.yaml
   ```

## 3. Sensory Target Prediction (Beer Project)

The synthetic data generated by the GLLM can be used for various downstream tasks depending on the application domain. In our case, we use it to predict sensory aroma profiles for novel beer combinations using a Conditional Variational Autoencoder (CVAE):

1. Navigate to the `prediction` folder:

   ```bash
   cd src/prediction
   ```

2. Train the CVAE model:

   ```bash
   python model_training.py
   ```
3. Generate predictions:

   ```bash
   python inference.py
   ```
